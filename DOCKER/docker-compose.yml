version: "3.9"

services:
  
  web: 
    image: webapp
    build:
      
      context: ../APP
      dockerfile: Dockerfile
    command: python manage.py runserver 0.0.0.0:8000
    working_dir: /app
    ports: 
      - '8000:8000'
    depends_on:
      - mysql
      - web-migrate
    restart: "unless-stopped"
    environment:
      - WAIT_HOSTS=mysql:3306
      - MYSQL_HOST=mysql
      - ENVIRONMENT=docker
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_DB=${MYSQL_DB}
      - MYSQL_PORT=${MYSQL_PORT}
  
      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}

      # FOR S3 BUCKET
      - ACCESS_KEY=${ACCESS_KEY}
      - ACCESS_SECRET=${ACCESS_SECRET}
      - BUCKET_NAME=${BUCKET_NAME}

    volumes: 
      - type: bind
        source: ../APP
        target: /app

  web-migrate: 
    image: webapp
    
    command: python manage.py migrate --noinput
    working_dir: /app
    depends_on:
      - mysql
    restart: "no"
    environment:
      - WAIT_HOSTS=mysql:3306
      - MYSQL_HOST=mysql
      - ENVIRONMENT=docker
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_DB=${MYSQL_DB}
      - MYSQL_PORT=${MYSQL_PORT}
  
      - DJANGO_SECRET_KEY=${DJANGO_SECRET_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}

      # FOR S3 BUCKET
      - ACCESS_KEY=${ACCESS_KEY}
      - ACCESS_SECRET=${ACCESS_SECRET}
      - BUCKET_NAME=${BUCKET_NAME}
  
    volumes: 
      - type: bind
        source: ../APP
        target: /app
    
  mysql:
    image: mysql
    volumes:
      - mysql-data:/var/lib/mysql
    environment:
      # Database name
      MYSQL_DATABASE: ${MYSQL_DB}
      # So you don't have to use root, but you can if you like
      MYSQL_USER: ${MYSQL_USER}
      # You can use whatever password you like
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
      # Password for root access
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_ADMIN_USER: ${MYSQL_ADMIN_USER}
      CHARSET: utf8
    restart: "unless-stopped"
    ports:
      - '3306:3306'
  
  myphpadmin:
    image: phpmyadmin/phpmyadmin
    ports:
      - 5050:80
    environment:
      PMA_HOST: mysql
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
    restart: "unless-stopped"
  # cache:
  #   image: redis:6.2-alpine
  #   restart: "unless-stopped"
  #   ports:
  #     - '6379:6379'
  #   command: redis-server --save 20 1 --loglevel warning 
  #   volumes: 
  #     - cache:/data
  warehouse-api:
    build:
      context: ../WAREHOUSE/
      dockerfile: Dockerfile
    restart: "unless-stopped"
    ports:
      - '8080:8000'
    environment:
      - REDIS_HOST=cache
      - REDIS_PORT=6379
    volumes:
      - ../tmp_data/data:/data
      - ../WAREHOUSE:/code/API
  postgres:
    image: postgres:12
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: "unless-stopped"
  airflow-webserver:
    image: apache/airflow:latest-python3.10
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__WEBSERVER__BASE_URL=${AIRFLOW__WEBSERVER__BASE_URL}
      - AIRFLOW__WEBSERVER__AUTHENTICATE=${AIRFLOW__WEBSERVER__AUTHENTICATE}
      - AIRFLOW__WEBSERVER__AUTH_BACKEND=${AIRFLOW__WEBSERVER__AUTH_BACKEND}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}

      - WAREHOUSE_POSTGRES_HOST=warehouse-postgres
      - WAREHOUSE_POSTGRES_PORT=${WAREHOUSE_POSTGRES_PORT}
      - WAREHOUSE_POSTGRES_NAME=${WAREHOUSE_POSTGRES_NAME}
      - WAREHOUSE_POSTGRES_USER=${WAREHOUSE_POSTGRES_USER}
      - WAREHOUSE_POSTGRES_PASSWORD=${WAREHOUSE_POSTGRES_PASSWORD}

    volumes:
      - ../AIRFLOW/dags:/opt/airflow/dags
      - ../AIRFLOW/logs:/opt/airflow/logs
      - ../AIRFLOW/plugins:/opt/airflow/plugins
      # - /var/run/docker.sock:/var/run/docker.sock

    ports:
      - "8081:8080"
    command: ["airflow", "webserver"]
    restart: "unless-stopped"

  airflow-setup:
    image: apache/airflow:latest-python3.10
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__WEBSERVER__BASE_URL=${AIRFLOW__WEBSERVER__BASE_URL}
      - AIRFLOW__WEBSERVER__AUTHENTICATE=${AIRFLOW__WEBSERVER__AUTHENTICATE}
      - AIRFLOW__WEBSERVER__AUTH_BACKEND=${AIRFLOW__WEBSERVER__AUTH_BACKEND}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
    command: >
      bash -c 'airflow db init &&
        airflow users create \
            --username $ADMIN_USERNAME \
            --password $ADMIN_PASSWORD \
            --firstname Admin \
            --lastname User \
            --role Admin \
            --email $ADMIN_EMAIL'
  airflow-scheduler:
    image: apache-scheduler
    build:
      context: ./AIRFLOW/docker/scheduler
      dockerfile: Dockerfile
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW__CORE__EXECUTOR}
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW__CORE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__LOAD_EXAMPLES=${AIRFLOW__CORE__LOAD_EXAMPLES}
      - AIRFLOW__WEBSERVER__BASE_URL=${AIRFLOW__WEBSERVER__BASE_URL}
      - AIRFLOW__WEBSERVER__AUTHENTICATE=${AIRFLOW__WEBSERVER__AUTHENTICATE}
      - AIRFLOW__WEBSERVER__AUTH_BACKEND=${AIRFLOW__WEBSERVER__AUTH_BACKEND}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_HOST=${POSTGRES_HOST}
      
      - WAREHOUSE_POSTGRES_HOST=warehouse-postgres
      - WAREHOUSE_POSTGRES_PORT=${WAREHOUSE_POSTGRES_PORT}
      - WAREHOUSE_POSTGRES_NAME=${WAREHOUSE_POSTGRES_NAME}
      - WAREHOUSE_POSTGRES_USER=${WAREHOUSE_POSTGRES_USER}
      - WAREHOUSE_POSTGRES_PASSWORD=${WAREHOUSE_POSTGRES_PASSWORD}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_DB=${MYSQL_DB}
      - MYSQL_PORT=${MYSQL_PORT}
      
    volumes:
      - ../AIRFLOW/dags:/opt/airflow/dags
      - ../AIRFLOW/logs:/opt/airflow/logs
      - ../AIRFLOW/plugins:/opt/airflow/plugins
      - ../tmp_data/data:/app/data
      - ../AIRFLOW/scripts:/app/scripts

    command: ["airflow", "scheduler"]
    restart: "unless-stopped"

  warehouse-postgres:
    image: postgres:12
    environment:
      POSTGRES_USER: ${WAREHOUSE_POSTGRES_USER}
      POSTGRES_PASSWORD: ${WAREHOUSE_POSTGRES_PASSWORD}
      POSTGRES_DB: ${WAREHOUSE_POSTGRES_DB}
    volumes:
      - warehouse_postgres_data:/var/lib/postgresql/data
    restart: "unless-stopped"

  # etl-image-build:
  #   image: my-etl-image
  #   build:
  #     context: ./docker/ETL
  #     dockerfile: Dockerfile
  #   volumes:
  #     - ./data:/app/data
  #     - ./scripts:/app/scripts
  #   command: "echo 'ETL image built'"
volumes:
  mysql-data:
  cache:
    driver: local
  postgres_data:
  warehouse_postgres_data: